{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Cluster.tools","text":"<p>Cluster.tools is an open-source orchestration and developer framework for simplifying the deployment of distributed etl functions. The simplicity comes from the rich set of deployment monitoring, high-availability, and auto-scaling tools provided out of the box.</p> <p></p> <pre><code>[!] updating the content on this site is currently in progress\n</code></pre>"},{"location":"#built-from-an-operator-developer-perspective","title":"Built from an Operator &amp; Developer Perspective","text":"<p>Cluster.tools was developed from an operators perspective. When developing functions distributed in a private, public, or hybrid network what is the easiest way to version manage, load balance, monitor, and control access to these functions.</p> <p>A standard interface definition using HTTP and JSON also allows you to create a wrapper on top of any software so that it can interface with cluster.tools. </p>"},{"location":"#what-are-some-use-cases","title":"What are some Use Cases?","text":"<ol> <li>I want to load balance requests across several distributed deployments.</li> <li>I want to dynamically control access to functions in case of unexpected behavior.</li> <li>I want to update one unit of code without affecting service to unrelated units.</li> <li>I want to track the amount of data processed and how long it took.</li> <li>I want a central location to receive logs from distributed processes.</li> <li>I want a central configuration management solution that can be distributed across functions.</li> </ol>"},{"location":"#why-did-you-start-this","title":"Why did you start this?","text":"<p>Cluster.tools was started in parallel with a separate project to handle deploying etl functions in golang. Over time it evolved into its own project that is continued to this day.</p>"},{"location":"in15min/","title":"Get Upto Speed in 15 Minutes","text":"<p>The cluster.tools core is a cloud orchestration tool for Lambda and ETL functions. The power of the core is through handling multiple functionalities that are required for cloud pipelines but distract developers from building the transformations functions that are most important for business or product requirements.</p> <p>[show core and processors]</p> <p>It is important to note that the core itself does not provision (execute) any code itself, but rather is responsible for distributing calls to executables called processors that are responsible for providing a common interface for the core to call code. The interface between the core and processor is an HTTP REST API that accepts data in a JSON format for sending/receiving content. The details of the interface are described in more detail within the processor page.</p> <p>[what is a cluster, how it works]</p> <p>[how modules isolate clusters]</p> <p>[versioning to support multiple versions]</p> <p>[mounting / unmounting]</p> <p>[provisioning]</p> <p>Some of the functionality includes: 1. Dynamic Registering of Processors 2. Encapsulation/Isolation of Code into Register-able Modules  3. Module Versioning 4. Dynamic Availability through Mounting and Unmounting 4. Remote Procedure Calls 5. Load Balancing Across Processors with a common registered module 3. Configuration Management 4. Short Term Caching 5. Statistic Monitoring</p>"},{"location":"in5min/","title":"Getting Started in 5 Minutes","text":"<p>The cluster.tools core is a cloud orchestration tool for Lambda and ETL functions. The process called the core is responsible for orchestrating a distributed number of processors. </p> <p>The processors are responsible for the compute while the core handles resource management.</p>"},{"location":"in5min/#before-we-begin","title":"Before We Begin","text":"<p>I intended to move this process towards installing pre-built binaries over building from source. In the meantime, the go install command creates a binary in the GOPATH bin so ensure the path is added to your environments PATH.</p> <pre><code>export PATH=$(go env GOPATH)/bin:$PATH\n</code></pre>"},{"location":"in5min/#installation","title":"Installation","text":"<pre><code># create a local copy of the threads\ngit clone https://github.com/GabeCordo/cluster-tools\ngo install\n\n# add $(go env GOPATH)/bin to your environment PATH\n\n# generate global files used by the threads when run\ncluster-tools init\n</code></pre>"},{"location":"in5min/#installing-a-processor","title":"Installing a Processor","text":"<pre><code># create a local copy of the threads\ngit clone https://github.com/GabeCordo/processor-framework\ngo install\n\n# generate global files used by the threads when run\nprocessor-framework init\n</code></pre>"},{"location":"in5min/#running-the-core","title":"Running the Core","text":"<pre><code>cluster-tools start\n</code></pre>"},{"location":"in5min/#running-the-processor","title":"Running the Processor","text":"<pre><code>processor-framework connect http://localhost:8137\n</code></pre>"},{"location":"processors/","title":"Processors","text":"<p>Processors are separate programs responsible for registering to a cluster.tools core and accepting incoming provision requests.</p> <p>The core and processor communicate by exchanging messages over HTTP REST endpoints encoding objects using JSON within the request or response body.</p> <p>Anyone can developer their own processor as long as it implements the interface defined by the core. This page documents the use cases and interfaces you can use as a guide to create your own processor.</p> <p>If you have implemented an interface for a new language and wish to add it to this page, raise a pull request on the wiki repository.</p> Language Developer Repository Template Golang Gabriel Cordovado https://github.com/GabeCordo/processor-framework goto"},{"location":"processors/#basic-implementation","title":"Basic Implementation","text":""},{"location":"processors/#how-does-the-processor-communicate-with-the-core","title":"How does the processor communicate with the core?","text":"<p>The processor must implement a series of functions to register itself and its callable functions with the core. Its the equipment of letting the core know you exist and what its allowed to ask you to run.</p>"},{"location":"processors/#connect-request","title":"Connect Request","text":"<p>When the processor binary is started it shall create a new processor record on the core. The processor record holds two pieces of information, (1) the address the core should use when sending messages to the processor, (2) the port the processor is listening for incoming http requests.   </p>"},{"location":"processors/#post-httplocalhost8137processor","title":"POST http://localhost:8137/processor","text":"<pre><code>{\n    \"host\": \"localhost\",\n    \"port\": 5023\n}\n</code></pre>"},{"location":"processors/#register-module-request","title":"Register Module Request","text":"<p>When the processor binary is ready to receive requests from the core it shall create a module record. The module is a grouping of functions that share a common logical purpose. Modules also contain associative metadata like version and developer contact information so a user can better understand what they are using.</p> <p>In the example below, we are registering a module called common that contains one function Vec. We set the on-load specifier to batch signifying that it should be user callable.</p>"},{"location":"processors/#post-httplocalhost8137module","title":"POST http://localhost:8137/module","text":"<pre><code>{\n    \"host\": \"localhost\",\n    \"port\": 5023,\n    \"module\": {\n        \"config\": {\n            \"name\": \"common\",\n            \"version\": 1,\n            \"contact\": {\n                \"name\": \"James Bond\",\n                \"email\": \"james.bond@gmail.com\"\n            },\n            \"exports\": [\n                {\n                \"cluster\": \"Vec\",\n                \"mount\": true,\n                \"config\": {\n                    \"mode\": \"Batch\",\n                    \"on-crash\": \"DoNothing\",\n                    \"on-load\": \"WaitAndPush\",\n                    \"static\": {\n                    \"t-functions\": 1,\n                    \"l-functions\": 1\n                    },\n                    \"dynamic\": {\n                    \"t-function\": {\n                        \"threshold\": 2,\n                        \"growth-factor\": 2\n                    },\n                    \"l-function\": {\n                        \"threshold\": 2,\n                        \"growth-factor\": 2\n                    }\n                    }\n                }\n                }\n            ]\n        }\n    }\n}\n</code></pre>"},{"location":"processors/#sending-updates-to-the-core","title":"Sending Updates to the Core","text":"<p>TODO</p>"},{"location":"processors/#how-does-the-core-communicate-with-the-processor","title":"How does the core communicate with the processor?","text":"<p>The processor must implement an HTTP endpoint to receive run requests from the core.</p>"},{"location":"processors/#incoming-run-request","title":"Incoming Run Request","text":""},{"location":"processors/#post-httpprocessor-urlsupervisor","title":"POST http://processor-url/supervisor","text":"<pre><code>{\n    \"module\": \"name\",\n    \"cluster\": \"function-name\",\n    \"config\": \"config-name\",\n    \"supervisor\": 0, // reference # of the run\n    \"metadata\": {\n        \"key\": \"value\"  // per-run parameters\n    }\n}\n</code></pre>"},{"location":"processors/#advanced-implementation","title":"Advanced Implementation","text":"<p>TODO</p>"},{"location":"processors/#logging","title":"Logging","text":"<p>TODO</p>"},{"location":"processors/#caching","title":"Caching","text":"<p>TODO</p>"},{"location":"concepts/mounting/","title":"Mounting","text":"<p>The term mounting is synonymous with act of making a resource available for provisioning. The term comes from the idea of mounting a server, when the server is mounted connections can begin inbound, when a server is unmounted the connections will cease. </p> <p>We choose to mount or unmount a resource based on whether we think it is a good idea to allow inbound connections. When a resource is being decommissioned or the number of faults is greater than our uptime KPI it may be beneficial to unmount a resource.</p> <p>Since the core is a facade for provisioning resources it has the authority to decide whether to send the provision request to a processor or drop it.</p>"},{"location":"concepts/mounting/#mountable-resources","title":"Mountable Resources","text":""},{"location":"concepts/mounting/#module","title":"Module","text":"<p>When a module is mounted, provision requests may be directed to the cluster but the cluster must still be mounted. When a module is un-mounted, no clusters within the module may be provisioned.  </p>"},{"location":"concepts/mounting/#cluster","title":"Cluster","text":"<p>When a cluster is mounted, provision requests may be directed towards a supporting processor. The module the cluster belongs to must still be mounted to access the resource.</p>"},{"location":"concepts/mounting/#observability","title":"Observability","text":"<p>When a resource is mounted or un-mounted the updated status will appear in standard out.</p> <pre><code>\u251c\u2500 common (mounted: true) \n|  \u251c\u2500vec (mounted: true)\n|  |  \u251c\u2500localhost:5023\n|  \u251c\u2500hello (mounted: true)\n|  |  \u251c\u2500localhost:5023\n</code></pre>"},{"location":"concepts/overview/","title":"Cluster.tools Overview","text":"<p>The core acts as a facade for zero to many processors responsible for providing compute. The facade provides an operator the ability to manage resource, configurations, mount states, and provisions towards the processors.</p>"},{"location":"concepts/overview/#the-core-and-the-processor","title":"The core and the processor","text":"<p>The core is responsible for managing a distributed deployment while a processor is an instance in the deployment that provides compute to run a function.</p>"},{"location":"concepts/overview/#understanding-the-core","title":"Understanding the core","text":"<p>The core is responsible for holding the context of resources active in the deployment and facilitating communication between an operator and the resources. </p> <p>During startup the core begins by launching threads; a database to manage records, a cache to store temporary objects in memory, tables to manage</p> <p>The core allows and operator and processor to communicate with itself through exposed endpoints:</p> <ol> <li>The client endpoint</li> <li>The processor endpoint</li> </ol>"},{"location":"concepts/overview/#understanding-the-processor","title":"Understanding the processor","text":"<p>The processor is responsible for registering itself to the core on startup, allowing the core to manage its compute. The processor organizes the compute in modules and clusters. Modules contain a set of provisional functions clusters </p> <p></p> Figure 1. a core connect to two (2) processors p1 and p2"},{"location":"concepts/overview/#the-module","title":"The module","text":"<p>A module encapsulates common functional elements together and provides the opportunity to version and accredit its developers. </p>"},{"location":"concepts/overview/#its-all-a-set-of-clusters","title":"It's all a set of clusters","text":"<p>The smallest referable functional unit in cluster.tools is a cluster. A cluster represents a set of functions that make-up an extract transform load (etl) process whose functionality is invoked as one.</p> <p></p> Figure 2. a module wrapping two (2) clusters m1.A and m1.B  <p>Figure 2 shows two clusters m1.A and m1.B that represent unique etl sets where m1.A performs a transformation on data-type A and m1.B performs a transformation on data-type B. Both A and B are separate data-types but share a common functional use so they can be grouped together in module m1.</p> <p>The processor can choose what modules it wants to support at any point in its lifecycle, it is up to the processor to inform the core when it wants to support a module. Once the core is aware that a processor supports a module it can begin directing provisioning requests from the facade to the processor.</p>"},{"location":"concepts/overview/#provisioning-a-cluster","title":"Provisioning a cluster","text":""},{"location":"concepts/supervisor/","title":"Supervisors","text":"<p>The term supervisor refers to a process that is responsible for executing the etl functions that make up a cluster. We call it a supervisor because it is also responsible for monitoring the data flow between the functions and determining when resources are exhausted.</p>"},{"location":"concepts/supervisor/#defining-behaviors-with-configurations","title":"Defining behaviors with configurations","text":""},{"location":"concepts/supervisor/#the-initial-startup","title":"The initial startup","text":""},{"location":"concepts/supervisor/#the-flow-of-data","title":"The flow of data","text":""},{"location":"concepts/supervisor/#handling-resource-exhaustion","title":"Handling resource exhaustion","text":""},{"location":"concepts/architecture/components/cache/","title":"Cache","text":""},{"location":"concepts/architecture/components/cache/#cache","title":"Cache","text":"<p>The ETL Framework has a built-in solution for caching intermediate data that is used in a chain of sequential ETL clusters. This is an alternative to loading and storing data in an external database solution that introduces networking errors and latency.</p>"},{"location":"concepts/architecture/components/cache/#ensuring-the-cache-is-enabled","title":"Ensuring the Cache is Enabled","text":"<p>When starting up the ETL Framework on debug mode, you should see a system log notifying you that the cache thread has started.</p> <p><code>2023/01/25 12:37:45 (+) Cache Thread Started</code></p>"},{"location":"concepts/architecture/components/cache/#cache-lifetime","title":"Cache Lifetime","text":"<p>Cache data is intended for short lifetime storage. This is to handle that the cache resides in system memory (which is usually limited) and that permanent or long-term storage of data should be left to other database solutions.</p> <p>By Default Cache Records Have a Lifetime of 1 Minute</p>"},{"location":"concepts/architecture/components/cache/#modifying-the-cache-lifetime","title":"Modifying the Cache Lifetime","text":"<p>The ETLConfig contains am \"expire-in\" field which specifies how many minutes should pass before a record is automatically removed from the cache.</p> <pre><code>{\n   \"cache\": {\n      \"expire-in\": 1\n   }\n}\n</code></pre>"},{"location":"concepts/architecture/components/cache/#cache-memory-usage","title":"Cache Memory Usage","text":"<p>The cache resides in the system memory which means that the storage capabilities are highly limited versus traditional database solutions that use disk storage. It is important we keep track of the number of cached values we store on the ETL Cache in case developers don't realize how much data their clusters are generating, or it is being misused as a black box for data storage.</p> <p>By Default Cache Records Have a Limit of 1000 Records If we have records storing 1MB of data, that is already using 1GB of system memory if maxed out.</p>"},{"location":"concepts/architecture/components/cache/#modifying-the-memory-usage","title":"Modifying the Memory Usage","text":"<p>The ETLConfig contains a \"max-size\" field in the \"cache\" section to change the max number of records allowed.</p> <pre><code>{\n   \"cache\": {\n      \"max-size\": 1000\n   }\n}\n</code></pre>"},{"location":"concepts/architecture/endpoints/client/","title":"HTTP Client Endpoint","text":"<p>The <code>client endpoint</code> is a HTTP REST port for an operator to interact with the core while it is operational. The endpoints runs on port <code>8136</code> by default but can be changed using the standard config file.</p>"},{"location":"concepts/architecture/endpoints/client/#processors","title":"Processors","text":"<p>The endpoint returns all the processors connected to the core upon its call.</p> <pre><code>curl --location 'http://localhost:8136/processor'\n</code></pre>"},{"location":"concepts/architecture/endpoints/client/#modules","title":"Modules","text":"<p>The endpoint returns all the modules registered to the core upon its call.</p> <pre><code>curl --location 'http://localhost:8136/module'\n</code></pre>"},{"location":"concepts/architecture/endpoints/client/#clusters","title":"Clusters","text":"<p>The endpoint shall return all clusters belonging to a module.</p> <pre><code>curl --location 'http://localhost:8136/cluster?module=common'\n</code></pre>"},{"location":"concepts/architecture/endpoints/client/#mounting","title":"Mounting","text":"<p>The endpoint shall allow an operator to modify the mounted state of a module. More information on mounting can be found here.</p> <pre><code>curl --location --request PUT 'http://localhost:8136/module' \\\n--header 'Content-Type: application/json' \\\n--data '{\n    \"module\": \"common\",\n    \"mounted\": true\n}'\n</code></pre> <pre><code>curl --location --request PUT 'http://localhost:8136/cluster' \\\n--header 'Content-Type: application/json' \\\n--data '{\n    \"module\": \"common\",\n    \"cluster\": \"Vec\",\n    \"mounted\": true\n}'\n</code></pre>"},{"location":"concepts/architecture/endpoints/client/#viewing-configs","title":"Viewing Configs","text":"<p>The endpoint shall returns all the configs associated with a module and identifier.</p> <pre><code>curl --location 'http://localhost:8136/config?module=common&amp;config=vec'\n</code></pre>"},{"location":"concepts/architecture/endpoints/client/#creating-configs","title":"Creating Configs","text":"<p>The endpoint shall permit the operator to create a new config while the core is operational.</p> <pre><code>curl --location 'http://localhost:8136/config?module=common' \\\n--header 'Content-Type: application/json' \\\n--data '{\n    \"identifier\": \"Random\",\n    \"on-load\": \"CompleteAndPush\",\n    \"on-crash\": \"DoNothing\",\n    \"start-with-n-t-channels\": 3,\n    \"start-with-n-l-channels\": 3,\n    \"et-channel-threshold\": 2,\n    \"et-channel-growth-factor\": 3,\n    \"tl-channel-threshold\": 2,\n    \"tl-channel-growth-factor\": 3\n}'\n</code></pre>"},{"location":"concepts/architecture/endpoints/client/#modifying-configs","title":"Modifying Configs","text":"<p>The endpoint shall permit the operator to dynamically modify the config while the core is operational.</p> <pre><code>curl --location --request PUT 'http://localhost:8136/config' \\\n--header 'Content-Type: application/json' \\\n--data '{\n    \"identifier\": \"Random\",\n    \"on-crash\": 0,\n    \"start-with-n-t-channels\": 5,\n    \"start-with-n-l-channels\": 3,\n    \"et-channel-threshold\": 2,\n    \"et-channel-growth-factor\": 3,\n    \"tl-channel-threshold\": 2,\n    \"tl-channel-growth-factor\": 3\n}'\n</code></pre>"},{"location":"concepts/architecture/endpoints/client/#deleting-configs","title":"Deleting Configs","text":"<p>The endpoint shall permit the operator to delete unused configs while the core is operational.</p> <pre><code>curl --location --request DELETE 'http://127.0.0.1:8136/config?module=common&amp;config=Random'\n</code></pre>"},{"location":"concepts/architecture/endpoints/client/#provisioning","title":"Provisioning","text":"<p>The endpoint shall permit an operator to request the execution of a cluster's code.</p> <pre><code>curl --location 'http://localhost:8136/supervisor' \\\n--header 'Content-Type: application/json' \\\n--data '{\n    \"module\": \"common\",\n    \"cluster\": \"vec\",\n    \"config\": \"vec\",\n    \"metadata\": {\n        \"test\": \"hello there\"\n    }\n}'\n</code></pre>"},{"location":"concepts/architecture/endpoints/client/#supervisors","title":"Supervisors","text":"<p>The endpoint shall permit an operator to query the state of an executed cluster.</p> <pre><code>curl --location 'http://localhost:8136/supervisor?module=common&amp;cluster=vec'\n</code></pre>"},{"location":"concepts/architecture/endpoints/client/#statistics","title":"Statistics","text":"<p>The endpoint shall return the statistics of an executed cluster.</p> <pre><code>curl --location 'http://localhost:8136/statistics?module=common&amp;cluster=vec'\n</code></pre>"},{"location":"concepts/architecture/endpoints/client/#viewing-cron-jobs","title":"Viewing Cron Jobs","text":"<p>The endpoint shall return all the cron jobs associated with a module.</p> <pre><code>curl --location 'http://localhost:8136/job?module=common'\n</code></pre>"},{"location":"concepts/architecture/endpoints/client/#creating-cron-jobs","title":"Creating Cron Jobs","text":"<p>The endpoint shall create a cron job to execute a cluster's code.</p> <pre><code>curl --location 'http://localhost:8136/job' \\\n--header 'Content-Type: application/json' \\\n--data '{\n    \"identifier\": \"test\",\n    \"module\": \"common\",\n    \"cluster\": \"hello\",\n    \"config\": \"hello\",\n    \"interval\": {\n        \"minute\": 10,\n        \"hour\": 0,\n        \"day\": 0,\n        \"month\": 0\n    },\n    \"metadata\": {}\n}'\n</code></pre>"},{"location":"concepts/architecture/endpoints/client/#deleting-cron-jobs","title":"Deleting Cron Jobs","text":"<p>The endpoint shall permit the operator to delete an existing cron job.</p> <pre><code>curl --location --request DELETE 'http://localhost:8136/job?id=test'\n</code></pre>"},{"location":"concepts/architecture/endpoints/client/#toggling-debug","title":"Toggling Debug","text":"<pre><code>curl --location 'http://localhost:8136/debug' \\\n--header 'Content-Type: application/json' \\\n--data '{\n    \"action\": \"debug\"\n}'\n</code></pre>"},{"location":"concepts/architecture/endpoints/client/#shutting-down","title":"Shutting Down","text":"<pre><code>curl --location 'http://localhost:8136/debug' \\\n--header 'Content-Type: application/json' \\\n--data '{\n    \"action\": \"shutdown\"\n}'\n</code></pre>"},{"location":"concepts/architecture/endpoints/processor/","title":"HTTP Processor Endpoint","text":"<p>The <code>processor endpoint</code> is an HTTP REST port for a processor to communicate with the cluster-tools core. </p> <p>[!] The processor endpoint is NOT for operators, do NOT use this port unless you are implementing a processor.</p>"},{"location":"concepts/architecture/endpoints/processor/#registering-with-the-core","title":"Registering with the Core","text":""},{"location":"concepts/architecture/interfaces/cluster/","title":"Cluster","text":""}]}